#!/usr/bin/env python3
"""
AWS Glue ETL Job: S3 CSV to Aurora PostgreSQL
L√™ arquivo CSV do S3 e grava em tabela Aurora PostgreSQL
Usa conex√µes Glue e credenciais do Secrets Manager
"""

import sys
import boto3
import json
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# =============================================================================
# CONFIGURA√á√ÉO INICIAL
# =============================================================================

# Obter argumentos do job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'S3_INPUT_PATH',
    'TARGET_TABLE',
    'CONNECTION_NAME',
    'SECRET_NAME',
    'DATABASE_NAME',
    'SCHEMA_NAME'  # Novo par√¢metro para schema espec√≠fico
])

# Inicializar contextos Spark/Glue
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Configurar logging
logger = glueContext.get_logger()
logger.info(f"=== INICIANDO JOB ETL: {args['JOB_NAME']} ===")

# =============================================================================
# FUN√á√ïES AUXILIARES
# =============================================================================

def get_secret_value(secret_name: str, region: str = 'us-east-1') -> dict:
    """
    Recupera credenciais do AWS Secrets Manager
    """
    try:
        logger.info(f"Recuperando credenciais do secret: {secret_name}")
        
        # Cliente Secrets Manager
        secrets_client = boto3.client('secretsmanager', region_name=region)
        
        # Obter secret
        response = secrets_client.get_secret_value(SecretId=secret_name)
        secret_string = response['SecretString']
        
        # Parse JSON
        credentials = json.loads(secret_string)
        logger.info("‚úÖ Credenciais recuperadas com sucesso")
        
        return credentials
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao recuperar credenciais: {str(e)}")
        raise e

def detect_csv_separator(s3_path: str) -> str:
    """
    Detecta o separador do CSV baseado na extens√£o e conte√∫do
    """
    try:
        if s3_path.lower().endswith('.tsv'):
            return '\t'
        elif '|' in s3_path or 'glassdoor' in s3_path.lower():
            return '|'
        else:
            return ','
    except:
        return ','

def read_csv_from_s3(s3_path: str) -> DataFrame:
    """
    L√™ arquivo CSV do S3 usando Spark
    """
    try:
        logger.info(f"Lendo arquivo CSV do S3: {s3_path}")
        
        # Detectar separador automaticamente
        separator = detect_csv_separator(s3_path)
        logger.info(f"üìã Separador detectado: '{separator}'")
        
        # Ler CSV com infer√™ncia de schema
        df = spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .option("encoding", "UTF-8") \
            .option("sep", separator) \
            .option("quote", '"') \
            .option("escape", '"') \
            .option("multiline", "true") \
            .csv(s3_path)
        
        # Log informa√ß√µes do DataFrame
        row_count = df.count()
        col_count = len(df.columns)
        
        logger.info(f"‚úÖ CSV lido com sucesso:")
        logger.info(f"   - Linhas: {row_count:,}")
        logger.info(f"   - Colunas: {col_count}")
        logger.info(f"   - Schema: {df.schema}")
        
        # Mostrar primeiras linhas (para debug)
        logger.info("üìã Primeiras 5 linhas:")
        df.show(5, truncate=False)
        
        return df
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao ler CSV do S3: {str(e)}")
        raise e

def transform_data(df: DataFrame) -> DataFrame:
    """
    Aplica transforma√ß√µes nos dados (customize conforme necess√°rio)
    """
    try:
        logger.info("üîÑ Aplicando transforma√ß√µes nos dados...")
        
        # Verificar se coluna 'id' existe (com varia√ß√µes)
        column_names_lower = [col.lower() for col in df.columns]
        id_variations = ['id', 'id_', '_id', 'pk', 'primary_key', 'key']
        
        existing_id_col = None
        for variation in id_variations:
            if variation in column_names_lower:
                # Encontrar o nome original da coluna
                original_idx = column_names_lower.index(variation)
                existing_id_col = df.columns[original_idx]
                break
        
        if existing_id_col:
            logger.info(f"‚úÖ Coluna ID encontrada: '{existing_id_col}'")
            # Renomear para 'id' se necess√°rio
            if existing_id_col.lower() != 'id':
                df = df.withColumnRenamed(existing_id_col, 'id')
                logger.info(f"üîÑ Coluna '{existing_id_col}' renomeada para 'id'")
        else:
            logger.info("‚ö†Ô∏è Coluna 'id' n√£o encontrada. Criando ID incremental...")
            # Adicionar coluna ID incremental usando row_number()
            from pyspark.sql.window import Window
            
            # Criar window spec para row_number
            window_spec = Window.orderBy(monotonically_increasing_id())
            
            # Adicionar coluna ID incremental
            df = df.withColumn("id", row_number().over(window_spec))
            logger.info("‚úÖ Coluna 'id' criada com valores incrementais")
        
        # Aplicar outras transforma√ß√µes
        df_transformed = df \
            .withColumn("created_at", current_timestamp()) \
            .withColumn("updated_at", current_timestamp())
        
        # Validar se ID n√£o √© nulo (agora que garantimos que existe)
        initial_count = df_transformed.count()
        df_transformed = df_transformed.filter(col("id").isNotNull())
        final_count = df_transformed.count()
        
        if initial_count != final_count:
            logger.warning(f"‚ö†Ô∏è Removidas {initial_count - final_count} linhas com ID nulo")
        
        # Limpar nomes de colunas (remover espa√ßos, caracteres especiais)
        for old_col in df_transformed.columns:
            new_col = old_col.strip().lower().replace(" ", "").replace("-", "").replace(".", "_")
            # Remover caracteres especiais adicionais
            import re
            new_col = re.sub(r'[^\w]', '_', new_col)
            # Remover underscores m√∫ltiplos
            new_col = re.sub(r'+', '', new_col).strip('_')
            
            if old_col != new_col:
                df_transformed = df_transformed.withColumnRenamed(old_col, new_col)
                logger.info(f"üîÑ Coluna renomeada: '{old_col}' ‚Üí '{new_col}'")
        
        # Log informa√ß√µes sobre a coluna ID
        try:
            id_stats = df_transformed.agg(
                min("id").alias("min_id"),
                max("id").alias("max_id"),
                count("id").alias("count_id")
            ).collect()[0]
            
            logger.info(f"üìä Estat√≠sticas da coluna ID:")
            logger.info(f"   - ID m√≠nimo: {id_stats['min_id']}")
            logger.info(f"   - ID m√°ximo: {id_stats['max_id']}")
            logger.info(f"   - Total de registros: {id_stats['count_id']:,}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel calcular estat√≠sticas do ID: {str(e)}")
        
        logger.info(f"‚úÖ Transforma√ß√µes aplicadas. Linhas finais: {df_transformed.count():,}")
        
        # Log schema final
        logger.info("üìã Schema final:")
        for field in df_transformed.schema.fields:
            logger.info(f"   - {field.name}: {field.dataType}")
        
        return df_transformed
        
    except Exception as e:
        logger.error(f"‚ùå Erro nas transforma√ß√µes: {str(e)}")
        raise e

def write_to_aurora(df: DataFrame, connection_name: str, database_name: str, schema_name: str, table_name: str, credentials: dict, table_exists: bool = False):
    """
    Escreve DataFrame no Aurora PostgreSQL usando conex√£o Glue
    """
    try:
        logger.info(f"üìù Escrevendo dados na tabela Aurora: {database_name}.{schema_name}.{table_name}")
        
        # Construir JDBC URL
        jdbc_url = f"jdbc:postgresql://{credentials['host']}:{credentials['port']}/{database_name}"
        
        # Determinar modo de escrita
        write_mode = "append" if table_exists else "overwrite"
        logger.info(f"üìã Modo de escrita: {write_mode}")
        
        # Nome completo da tabela com schema
        full_table_name = f"{schema_name}.{table_name}"
        
        # Configura√ß√µes de escrita
        write_options = {
            "url": jdbc_url,
            "dbtable": full_table_name,
            "user": credentials['username'],
            "password": credentials['password'],
            "driver": "org.postgresql.Driver",
            # Configura√ß√µes de performance
            "batchsize": "1000",
            "isolationLevel": "READ_UNCOMMITTED"
        }
        
        # Escrever dados
        df.write \
            .format("jdbc") \
            .options(**write_options) \
            .mode(write_mode) \
            .save()
        
        logger.info(f"‚úÖ Dados escritos com sucesso na tabela {full_table_name}")
        
        # Log estat√≠sticas finais
        final_count = df.count()
        logger.info(f"üìä Total de registros inseridos: {final_count:,}")
        
        # Verificar dados na tabela ap√≥s inser√ß√£o
        verify_insertion(credentials, database_name, schema_name, table_name)
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao escrever no Aurora: {str(e)}")
        raise e

def verify_insertion(credentials: dict, database_name: str, schema_name: str, table_name: str):
    """
    Verifica se os dados foram inseridos corretamente
    """
    try:
        logger.info(f"üîç Verificando inser√ß√£o na tabela {schema_name}.{table_name}...")
        
        import psycopg2
        
        # Conectar ao PostgreSQL
        conn = psycopg2.connect(
            host=credentials['host'],
            port=credentials['port'],
            database=database_name,
            user=credentials['username'],
            password=credentials['password']
        )
        
        cursor = conn.cursor()
        
        # Contar registros na tabela
        cursor.execute(f"SELECT COUNT(*) FROM {schema_name}.{table_name};")
        total_count = cursor.fetchone()[0]
        
        # Verificar primeiros registros
        cursor.execute(f"SELECT * FROM {schema_name}.{table_name} LIMIT 5;")
        sample_records = cursor.fetchall()
        
        # Obter nomes das colunas
        cursor.execute(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = '{schema_name}' 
            AND table_name = '{table_name}' 
            ORDER BY ordinal_position;
        """)
        column_names = [row[0] for row in cursor.fetchall()]
        
        logger.info(f"‚úÖ Verifica√ß√£o conclu√≠da:")
        logger.info(f"   - Total de registros na tabela: {total_count:,}")
        logger.info(f"   - Colunas na tabela: {len(column_names)}")
        logger.info(f"   - Nomes das colunas: {', '.join(column_names)}")
        
        if sample_records:
            logger.info("üìã Primeiros 5 registros inseridos:")
            for i, record in enumerate(sample_records, 1):
                logger.info(f"   Registro {i}: {record[:3]}...")  # Mostrar apenas primeiros 3 campos
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao verificar inser√ß√£o: {str(e)}")

def create_schema_if_not_exists(credentials: dict, database_name: str, schema_name: str):
    """
    Cria schema no Aurora se n√£o existir
    """
    try:
        logger.info(f"üîç Verificando se schema {schema_name} existe...")
        
        import psycopg2
        
        # Conectar ao PostgreSQL
        conn = psycopg2.connect(
            host=credentials['host'],
            port=credentials['port'],
            database=database_name,
            user=credentials['username'],
            password=credentials['password']
        )
        
        cursor = conn.cursor()
        
        # Verificar se schema existe
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.schemata 
                WHERE schema_name = %s
            );
        """, (schema_name,))
        
        schema_exists = cursor.fetchone()[0]
        
        if not schema_exists:
            logger.info(f"üìã Criando schema {schema_name}...")
            
            # Criar schema
            create_schema_sql = f"CREATE SCHEMA IF NOT EXISTS {schema_name};"
            cursor.execute(create_schema_sql)
            conn.commit()
            
            logger.info(f"‚úÖ Schema {schema_name} criado com sucesso")
        else:
            logger.info(f"‚úÖ Schema {schema_name} j√° existe")
        
        cursor.close()
        conn.close()
        
        return schema_exists
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar/criar schema: {str(e)}")
        # N√£o falhar o job se n√£o conseguir criar o schema
        logger.warning("‚ö†Ô∏è Continuando sem criar schema. Certifique-se que ele existe.")
        return False

def create_table_if_not_exists(credentials: dict, database_name: str, schema_name: str, table_name: str, df: DataFrame):
    """
    Cria tabela no Aurora se n√£o existir (baseado no schema do DataFrame)
    """
    try:
        logger.info(f"üîç Verificando se tabela {schema_name}.{table_name} existe...")
        
        import psycopg2
        
        # Conectar ao PostgreSQL
        conn = psycopg2.connect(
            host=credentials['host'],
            port=credentials['port'],
            database=database_name,
            user=credentials['username'],
            password=credentials['password']
        )
        
        cursor = conn.cursor()
        
        # Verificar se tabela existe no schema espec√≠fico
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = %s 
                AND table_name = %s
            );
        """, (schema_name, table_name))
        
        table_exists = cursor.fetchone()[0]
        
        if not table_exists:
            logger.info(f"üìã Criando tabela {schema_name}.{table_name}...")
            
            # Gerar DDL baseado no schema do DataFrame
            columns_ddl = []
            
            # Garantir que ID seja a primeira coluna e seja PRIMARY KEY
            id_added = False
            for field in df.schema.fields:
                col_name = field.name.lower()
                col_type = field.dataType
                
                # Mapear tipos Spark para PostgreSQL
                if isinstance(col_type, StringType):
                    pg_type = "TEXT"
                elif isinstance(col_type, IntegerType):
                    pg_type = "INTEGER"
                elif isinstance(col_type, LongType):
                    pg_type = "BIGINT"
                elif isinstance(col_type, DoubleType):
                    pg_type = "DOUBLE PRECISION"
                elif isinstance(col_type, BooleanType):
                    pg_type = "BOOLEAN"
                elif isinstance(col_type, TimestampType):
                    pg_type = "TIMESTAMP"
                elif isinstance(col_type, DateType):
                    pg_type = "DATE"
                else:
                    pg_type = "TEXT"  # Default
                
                # Se for coluna ID, adicionar como PRIMARY KEY
                if col_name == 'id':
                    columns_ddl.insert(0, f"{col_name} {pg_type} PRIMARY KEY")
                    id_added = True
                else:
                    columns_ddl.append(f"{col_name} {pg_type}")
            
            # Se n√£o encontrou coluna ID, adicionar uma
            if not id_added:
                columns_ddl.insert(0, "id BIGINT PRIMARY KEY")
                logger.info("‚ö†Ô∏è Adicionando coluna ID como PRIMARY KEY na tabela")
            
            # Criar tabela no schema espec√≠fico
            create_table_sql = f"""
                CREATE TABLE {schema_name}.{table_name} (
                    {', '.join(columns_ddl)}
                );
            """
            
            logger.info(f"üìù DDL da tabela: {create_table_sql}")
            cursor.execute(create_table_sql)
            conn.commit()
            
            logger.info(f"‚úÖ Tabela {schema_name}.{table_name} criada com sucesso")
        else:
            logger.info(f"‚úÖ Tabela {schema_name}.{table_name} j√° existe")
            
            # Verificar se precisa limpar dados existentes
            cursor.execute(f"SELECT COUNT(*) FROM {schema_name}.{table_name};")
            existing_count = cursor.fetchone()[0]
            logger.info(f"üìä Registros existentes na tabela: {existing_count:,}")
        
        cursor.close()
        conn.close()
        
        return table_exists
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar/criar tabela: {str(e)}")
        # N√£o falhar o job se n√£o conseguir criar a tabela
        logger.warning("‚ö†Ô∏è Continuando sem criar tabela. Certifique-se que ela existe.")
        return False

# =============================================================================
# EXECU√á√ÉO PRINCIPAL
# =============================================================================

def main():
    """
    Fun√ß√£o principal do ETL
    """
    try:
        logger.info("üöÄ Iniciando processo ETL S3 ‚Üí Aurora PostgreSQL")
        
        # 1. Recuperar credenciais do Secrets Manager
        logger.info("1Ô∏è‚É£ Recuperando credenciais...")
        credentials = get_secret_value(args['SECRET_NAME'])
        
        # 2. Ler dados do S3
        logger.info("2Ô∏è‚É£ Lendo dados do S3...")
        df_raw = read_csv_from_s3(args['S3_INPUT_PATH'])
        
        # 3. Aplicar transforma√ß√µes
        logger.info("3Ô∏è‚É£ Aplicando transforma√ß√µes...")
        df_transformed = transform_data(df_raw)
        
        # 4. Criar schema se n√£o existir
        logger.info("4Ô∏è‚É£ Verificando/criando schema...")
        create_schema_if_not_exists(credentials, args['DATABASE_NAME'], args['SCHEMA_NAME'])
        
        # 5. Criar tabela se n√£o existir
        logger.info("5Ô∏è‚É£ Verificando/criando tabela...")
        table_exists = create_table_if_not_exists(credentials, args['DATABASE_NAME'], args['SCHEMA_NAME'], args['TARGET_TABLE'], df_transformed)
        
        # 6. Escrever dados no Aurora
        logger.info("6Ô∏è‚É£ Escrevendo dados no Aurora...")
        write_to_aurora(df_transformed, args['CONNECTION_NAME'], args['DATABASE_NAME'], args['SCHEMA_NAME'], args['TARGET_TABLE'], credentials, table_exists)
        
        logger.info("üéâ ETL conclu√≠do com sucesso!")
        
        # Estat√≠sticas finais
        logger.info("üìä ESTAT√çSTICAS FINAIS:")
        logger.info(f"   - Arquivo S3: {args['S3_INPUT_PATH']}")
        logger.info(f"   - Schema destino: {args['SCHEMA_NAME']}")
        logger.info(f"   - Tabela destino: {args['DATABASE_NAME']}.{args['SCHEMA_NAME']}.{args['TARGET_TABLE']}")
        logger.info(f"   - Registros processados: {df_transformed.count():,}")
        
    except Exception as e:
        logger.error(f"üí• ERRO CR√çTICO no ETL: {str(e)}")
        import traceback
        logger.error(f"Stack trace: {traceback.format_exc()}")
        raise e

# =============================================================================
# EXECU√á√ÉO
# =============================================================================

if _name_ == "_main_":
    try:
        main()
    finally:
        # Finalizar job
        job.commit()
        logger.info("‚úÖ Job finalizado")